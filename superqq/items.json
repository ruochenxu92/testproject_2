[{"urllink": "http://arxiv.org/abs/1502.05957", "category": "Computation and Language ", "pdflink": "http://arxiv.org/pdf/1502.05957", "title": "\nWeb Similarity", "abstract": "> Normalized web distance (NWD) is a similarity or normalized semantic distance\nbased on the World Wide Web or any other large electronic database, for\ninstance Wikipedia, and a search engine that returns reliable aggregate page\ncounts. For sets of search terms the NWD gives a similarity on a scale from 0\n(identical) to 1 (completely different). The NWD approximates the similarity\naccording to all (upper semi)computable properties. We develop the theory and\ngive applications. The derivation of the NWD method is based on Kolmogorov\ncomplexity.\n", "subjects": "Information Retrieval (cs.IR)", "authors": "Andrew R. Cohen, Paul M.B. Vitanyi,", "date": "2015-02-24 01:36:14.853187"}, 
{"urllink": "http://arxiv.org/abs/1502.05888", "category": "Artificial Intelligence ", "pdflink": "http://arxiv.org/pdf/1502.05888", "title": "\nMajority-preserving judgment aggregation rules", "abstract": "> The literature on judgment aggregation has now been moving from studying\nimpossibility results regarding aggregation rules towards studying specific\njudgment aggregation rules. Here we focus on a family of rules that is the\nnatural counterpart of the family of Condorcet-consistent voting rules:\nmajority-preserving judgment aggregation rules. A judgment aggregation rule is\nmajority-preserving if whenever issue-wise majority is consistent, the rule\nshould output the majoritarian opinion on each issue. We provide a formal\nsetting for relating judgment aggregation rules to voting rules and propose\nsome basic properties for judgment aggregation rules. We consider six such\nrules some of which have already appeared in the literature, some others are\nnew. For these rules we consider their relation to known voting rules, to each\nother, with respect to their discriminating power, and analyse them with\nrespect to the considered properties.\n", "subjects": "Artificial Intelligence (cs.AI)", "authors": "Jer\u00f4me Lang, Gabriella Pigozzi, Marija Slavkovik, Leendert van der Torre, Srdjan Vesic,", "date": "2015-02-24 01:36:14.965969"}, 
{"urllink": "http://arxiv.org/abs/1502.05938", "category": "Computational Engineering, Finance, and Science ", "pdflink": "http://arxiv.org/pdf/1502.05938", "title": "\nIncorporating Spontaneous Reporting System Data to Aid Causal Inference  in Longitudinal Healthcare Data", "abstract": "> Inferring causality using longitudinal observational databases is challenging\ndue to the passive way the data are collected. The majority of associations\nfound within longitudinal observational data are often non-causal and occur due\nto confounding.\n<br>The focus of this paper is to investigate incorporating information from\nadditional databases to complement the longitudinal observational database\nanalysis. We investigate the detection of prescription drug side effects as\nthis is an example of a causal relationship. In previous work a framework was\nproposed for detecting side effects only using longitudinal data. In this paper\nwe combine a measure of association derived from mining a spontaneous reporting\nsystem database to previously proposed analysis that extracts domain expertise\nfeatures for causal analysis of a UK general practice longitudinal database.\n<br>The results show that there is a significant improvement to the performance\nof detecting prescription drug side effects when the longitudinal observation\ndata analysis is complemented by incorporating additional drug safety sources\ninto the framework. The area under the receiver operating characteristic curve\n(AUC) for correctly classifying a side effect when other data were considered\nwas 0.967, whereas without it the AUC was 0.923 However, the results of this\npaper may be biased by the evaluation and future work should overcome this by\ndeveloping an unbiased reference set.\n", "subjects": "Computational Engineering, Finance, and Science (cs.CE)", "authors": "Jenna Reps, Uwe Aickelin,", "date": "2015-02-24 01:36:15.006521"}, 
{"urllink": "http://arxiv.org/abs/1502.05912", "category": "Computational Complexity ", "pdflink": "http://arxiv.org/pdf/1502.05912", "title": "\nLimitations of Algebraic Approaches to Graph Isomorphism Testing", "abstract": "> We investigate the power of graph isomorphism algorithms based on algebraic\nreasoning techniques like Gr\\\"obner basis computation. The idea of these\nalgorithms is to encode two graphs into a system of equations that are\nsatisfiable if and only if if the graphs are isomorphic, and then to (try to)\ndecide satisfiability of the system using, for example, the Gr\\\"obner basis\nalgorithm. In some cases this can be done in polynomial time, in particular, if\nthe equations admit a bounded degree refutation in an algebraic proof systems\nsuch as Nullstellensatz or polynomial calculus. We prove linear lower bounds on\nthe polynomial calculus degree over all fields of characteristic different from\n2 and also linear lower bounds for the degree of Positivstellensatz calculus\nderivations.\n<br>We compare this approach to recently studied linear and semidefinite\nprogramming approaches to isomorphism testing, which are known to be related to\nthe combinatorial Weisfeiler-Lehman algorithm. We exactly characterise the\npower of the Weisfeiler-Lehman algorithm in terms of an algebraic proof system\nthat lies between degree-k Nullstellensatz and degree-k polynomial calculus.\n", "subjects": "Computational Complexity (cs.CC)", "authors": "Christoph Berkholz, Martin Grohe,", "date": "2015-02-24 01:36:14.897169"}, 
{"urllink": "http://arxiv.org/abs/1502.05883", "category": "Computer Science and Game Theory ", "pdflink": "http://arxiv.org/pdf/1502.05883", "title": "\nThe Efficient Frontier in Randomized Social Choice", "abstract": "> Since the celebrated Gibbard-Satterthwaite impossibility results and\nGibbard's 1977 extension for randomized rules, it is known that\nstrategyproofness imposes severe restrictions on the design of social choice\nrules. In this paper, we employ approximate strategyproofness and the notion of\nscore deficit to study the possible and necessary trade-offs between\nstrategyproofness and efficiency in the randomized social choice domain. In\nparticular, we analyze which social choice rules make optimal trade-offs, i.e.,\nwe analyze the efficient frontier. Our main result is that the efficient\nfrontier consists of two building blocks: (1) we identify a finite set of\n\"manipulability bounds\" B and the rules that are optimal at each of them; (2)\nfor all other bounds not in B, we show that the optimal rules at those bounds\nare mixtures of two rules that are optimal at the two nearest manipulability\nbounds from B. We provide algorithms that exploit this structure to identify\nthe entire efficient frontier for any given scoring function. Finally, we\nprovide applications of our results to illustrate the structure of the\nefficient frontier for the scoring functions v=(1,0,0) and v=(1,1,0).\n", "subjects": "Computer Science and Game Theory (cs.GT)", "authors": "Timo Mennle, Sven Seuken,", "date": "2015-02-24 01:36:15.251394"}, 
{"urllink": "http://arxiv.org/abs/1502.05558", "category": "Computational Geometry ", "pdflink": "http://arxiv.org/pdf/1502.05558", "title": "\nIt's a Tough Nanoworld: in Tile Assembly, Cooperation is not (strictly)  more Powerful than Competition", "abstract": "> We present a strict separation between the class of \"mismatch free\"\nself-assembly systems and general aTAM systems. Mismatch free systems are those\nsystems in which concurrently grown parts must always agree with each other.\n<br>Tile self-assembly is a model of the formation of crystal growth, in which a\nlarge number of particles concurrently and selectively stick to each other,\nforming complex shapes and structures. It is useful in nanotechnologies, and\nmore generally in the understanding of these processes, ubiquitous in natural\nsystems.\n<br>The other property of the local assembly process known to change the power of\nthe model is cooperation between two tiles to attach another. We show that\ndisagreement (mismatches) and cooperation are incomparable: neither can be used\nto simulate the other one.\n<br>The fact that mismatches are a hard property is especially surprising, since\nno known, explicit construction of a computational device in tile assembly uses\nmismatches, except for the recent construction of an intrinsically universal\ntileset, i.e. a tileset capable of simulating any other tileset up to\nrescaling. This work shows how to use intrinsic universality in a systematic\nway to highlight the essence of different features of tile assembly.\n<br>Moreover, even the most recent experimental realizations do not use\ncompetition, which, in view of our results, suggests that a large part of the\nnatural phenomena behind DNA self-assembly remains to be understood\nexperimentally.\n", "subjects": "Computational Geometry (cs.CG)", "authors": "Florent Becker, Pierre-\u00c9tienne Meunier,", "date": "2015-02-24 01:36:15.268152"}, 
{"urllink": "http://arxiv.org/abs/1502.05191", "category": "Computers and Society ", "pdflink": "http://arxiv.org/pdf/1502.05191", "title": "\nDetermining Training Needs for Cloud Infrastructure Investigations using  I-STRIDE", "abstract": "> As more businesses and users adopt cloud computing services, security\nvulnerabilities will be increasingly found and exploited. There are many\ntechnological and political challenges where investigation of potentially\ncriminal incidents in the cloud are concerned. Security experts, however, must\nstill be able to acquire and analyze data in a methodical, rigorous and\nforensically sound manner. This work applies the STRIDE asset-based risk\nassessment method to cloud computing infrastructure for the purpose of\nidentifying and assessing an organization's ability to respond to and\ninvestigate breaches in cloud computing environments. An extension to the\nSTRIDE risk assessment model is proposed to help organizations quickly respond\nto incidents while ensuring acquisition and integrity of the largest amount of\ndigital evidence possible. Further, the proposed model allows organizations to\nassess the needs and capacity of their incident responders before an incident\noccurs.\n", "subjects": "Computers and Society (cs.CY)", "authors": "Joshua I. James, Ahmed F. Shosha, Pavel Gladyshev,", "date": "2015-02-24 01:36:15.515274"}, 
{"urllink": "http://arxiv.org/abs/1502.05928", "category": "Computer Vision and Pattern Recognition ", "pdflink": "http://arxiv.org/pdf/1502.05928", "title": "\nSupervised Dictionary Learning and Sparse Representation-A Review", "abstract": "> Dictionary learning and sparse representation (DLSR) is a recent and\nsuccessful mathematical model for data representation that achieves\nstate-of-the-art performance in various fields such as pattern recognition,\nmachine learning, computer vision, and medical imaging. The original\nformulation for DLSR is based on the minimization of the reconstruction error\nbetween the original signal and its sparse representation in the space of the\nlearned dictionary. Although this formulation is optimal for solving problems\nsuch as denoising, inpainting, and coding, it may not lead to optimal solution\nin classification tasks, where the ultimate goal is to make the learned\ndictionary and corresponding sparse representation as discriminative as\npossible. This motivated the emergence of a new category of techniques, which\nis appropriately called supervised dictionary learning and sparse\nrepresentation (S-DLSR), leading to more optimal dictionary and sparse\nrepresentation in classification tasks. Despite many research efforts for\nS-DLSR, the literature lacks a comprehensive view of these techniques, their\nconnections, advantages and shortcomings. In this paper, we address this gap\nand provide a review of the recently proposed algorithms for S-DLSR. We first\npresent a taxonomy of these algorithms into six categories based on the\napproach taken to include label information into the learning of the dictionary\nand/or sparse representation. For each category, we draw connections between\nthe algorithms in this category and present a unified framework for them. We\nthen provide guidelines for applied researchers on how to represent and learn\nthe building blocks of an S-DLSR solution based on the problem at hand. This\nreview provides a broad, yet deep, view of the state-of-the-art methods for\nS-DLSR and allows for the advancement of research and development in this\nemerging area of research.\n", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "authors": "Mehrdad J. Gangeh, Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel,", "date": "2015-02-24 01:36:15.545435"}, 
{"urllink": "http://arxiv.org/abs/1502.05448", "category": "Cryptography and Security ", "pdflink": "http://arxiv.org/pdf/1502.05448", "title": "\nDistributed Inference in the Presence of Eavesdroppers: A Survey", "abstract": "> The distributed inference framework comprises of a group of spatially\ndistributed nodes which acquire observations about a phenomenon of interest.\nDue to bandwidth and energy constraints, the nodes often quantize their\nobservations into a finite-bit local message before sending it to the fusion\ncenter (FC). Based on the local summary statistics transmitted by nodes, the FC\nmakes a global decision about the presence of the phenomenon of interest. The\ndistributed and broadcast nature of such systems makes them quite vulnerable to\ndifferent types of attacks. This paper addresses the problem of secure\ncommunication in the presence of eavesdroppers. In particular, we focus on\nefficient mitigation schemes to mitigate the impact of eavesdropping. We\npresent an overview of the distributed inference schemes under secrecy\nconstraints and describe the currently available approaches in the context of\ndistributed detection and estimation followed by a discussion on avenues for\nfuture research.\n", "subjects": "Cryptography and Security (cs.CR)", "authors": "Bhavya Kailkhura, V. Sriram Siddhardh Nadendla, Pramod K. Varshney,", "date": "2015-02-24 01:36:15.771080"}, 
{"urllink": "http://arxiv.org/abs/1502.05937", "category": "Data Structures and Algorithms ", "pdflink": "http://arxiv.org/pdf/1502.05937", "title": "\nComposite repetition-aware data structures", "abstract": "> In highly repetitive strings, like collections of genomes from the same\nspecies, distinct measures of repetition all grow sublinearly in the length of\nthe text, and indexes targeted to such strings typically depend only on one of\nthese measures. We describe two data structures whose size depends on multiple\nmeasures of repetition at once, and that provide competitive tradeoffs between\nthe time for counting and reporting all the exact occurrences of a pattern, and\nthe space taken by the structure. The key component of our constructions is the\nrun-length encoded BWT (RLBWT), which takes space proportional to the number of\nBWT runs: rather than augmenting RLBWT with suffix array samples, we combine it\nwith data structures from LZ77 indexes, which take space proportional to the\nnumber of LZ77 factors, and with the compact directed acyclic word graph\n(CDAWG), which takes space proportional to the number of extensions of maximal\nrepeats. The combination of CDAWG and RLBWT enables also a new representation\nof the suffix tree, whose size depends again on the number of extensions of\nmaximal repeats, and that is powerful enough to support matching statistics and\nconstant-space traversal.\n", "subjects": "Data Structures and Algorithms (cs.DS)", "authors": "Djamal Belazzougui, Fabio Cunial, Travis Gagie, Nicola Prezza, Mathieu Raffinot,", "date": "2015-02-24 01:36:15.789494"}, 
{"urllink": "http://arxiv.org/abs/1502.05947", "category": "Databases ", "pdflink": "http://arxiv.org/pdf/1502.05947", "title": "\nFunctorial Data Migration: From Theory to Practice", "abstract": "> In this paper we describe a functorial data migration scenario about the\nmanufacturing service capability of a distributed supply chain. The scenario is\na category-theoretic analog of an OWL ontology-based semantic enrichment\nscenario developed at the National Institute of Standards and Technology\n(NIST). The scenario is presented using, and is included with, the open-source\nFQL tool, available for download at categoricaldata.net/fql.html.\n", "subjects": "Databases (cs.DB)", "authors": "Ryan Wisnesky, David I. Spivak, Patrick Schultz, Eswaran Subrahmanian,", "date": "2015-02-24 01:36:15.882960"}, 
{"urllink": "http://arxiv.org/abs/1502.05983", "category": "Discrete Mathematics ", "pdflink": "http://arxiv.org/pdf/1502.05983", "title": "\nSorting Networks: The Final Countdown", "abstract": "> In this paper we extend the knowledge on the problem of empirically searching\nfor sorting networks of minimal depth. We present new search space pruning\ntechniques for the last four levels of a candidate sorting network by\nconsidering only the output set representation of a network. We present an\nalgorithm for checking whether an $n$-input sorting network of depth $d$ exists\nby considering the minimal up to permutation and reflection itemsets at each\nlevel and using the pruning at the last four levels. We experimentally\nevaluated this algorithm to find the optimal depth sorting networks for all $n\n\\leq 12$.\n", "subjects": "Discrete Mathematics (cs.DM)", "authors": "Martin Marinov, David Gregg,", "date": "2015-02-24 01:36:16.168148"}, 
{"urllink": "http://arxiv.org/abs/1502.05701", "category": "Digital Libraries ", "pdflink": "http://arxiv.org/pdf/1502.05701", "title": "\nInterpreting \"altmetrics\": viewing acts on social media through the lens  of citation and social theories", "abstract": "> More than 30 years after Cronin's seminal paper on \"the need for a theory of\nciting\" (Cronin, 1981), the metrics community is once again in need of a new\ntheory, this time one for so-called \"altmetrics\". Altmetrics, short for\nalternative (to citation) metrics -- and as such a misnomer -- refers to a new\ngroup of metrics based (largely) on social media events relating to scholarly\ncommunication. As current definitions of altmetrics are shaped and limited by\nactive platforms, technical possibilities, and business models of aggregators\nsuch as Altmetric.com, ImpactStory, PLOS, and Plum Analytics, and as such\nconstantly changing, this work refrains from defining an umbrella term for\nthese very heterogeneous new metrics. Instead a framework is presented that\ndescribes acts leading to (online) events on which the metrics are based. These\nactivities occur in the context of social media, such as discussing on Twitter\nor saving to Mendeley, as well as downloading and citing. The framework groups\nvarious types of acts into three categories -- accessing, appraising, and\napplying -- and provides examples of actions that lead to visibility and\ntraceability online. To improve the understanding of the acts, which result in\nonline events from which metrics are collected, select citation and social\ntheories are used to interpret the phenomena being measured. Citation theories\nare used because the new metrics based on these events are supposed to replace\nor complement citations as indicators of impact. Social theories, on the other\nhand, are discussed because there is an inherent social aspect to the\nmeasurements.\n", "subjects": "Digital Libraries (cs.DL)", "authors": "Stefanie Haustein, Timothy D. Bowman, Rodrigo Costas,", "date": "2015-02-24 01:36:16.191646"}, 
{"urllink": "http://arxiv.org/abs/1502.05831", "category": "Distributed, Parallel, and Cluster Computing ", "pdflink": "http://arxiv.org/pdf/1502.05831", "title": "\nXFT: Practical fault tolerance beyond crashes", "abstract": "> Despite 30+ years of intensive research, the distributed computing community\nstill does not have a practical answer to non-crash faults of the machines that\ncomprise a distributed system. In particular, Byzantine fault-tolerance (BFT),\nthat promises to handle such faults, has not lived to expectations due to its\nresource and operation overhead with respect to its crash fault-tolerant(CFT)\ncounterparts. This overhead comes from the worst-case assumption about\nByzantine faults, in the sense that some coordinated adversarial activity\ncontrols the faulty machines and the entire network at will. To practitioners,\nhowever, such strong attacks appear irrelevant.\n<br>In this paper, we introduce XFT (\\cross fault tolerance\"), a novel approach\nto building reliable distributed systems, that decouples the fault space across\nthe machine and network faults dimensions, treating machine faults and network\nasynchrony separately. This is in sharp contrast to the existing CFT and BFT\nmodels that discern system faults only along the machine fault dimension. XFT\noffers much more flexibility than traditional synchronous and asynchronous\nmodels that (too strictly) fix the network fault model of interest regardless\nof the machine faults.\n<br>As the showcase for XFT, we present Paxos++: the first state machine\nreplication protocol in the XFT model. Paxos++ tolerates faults beyond crashes\nin an efficient and practical way, featuring many more nines of reliability\nthan the celebrated crash-tolerant Paxos protocol, without impacting its\nresource/operation costs while maintaining the same performance (common-case\ncommunication complexity among replicas). Surprisingly, Paxos++ sometimes\n(depending on the system environment) even offers strictly stronger reliability\nguarantees than state-of-the-art BFT replication protocols.\n", "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC)", "authors": "Shengyun Liu, Christian Cachin, Vivien Qu\u00e9ma, Marko Vukoli\u0107,", "date": "2015-02-24 01:36:16.293060"}, 
{"urllink": "http://arxiv.org/abs/1502.05873", "category": "Formal Languages and Automata Theory ", "pdflink": "http://arxiv.org/pdf/1502.05873", "title": "\nAnnotated Stack Trees", "abstract": "> Annotated pushdown automata provide an automaton model of higher-order\nrecursion schemes, which may in turn be used to model higher-order programs for\nthe purposes of verification. We study Ground Annotated Stack Tree Rewrite\nSystems -- a tree rewrite system where each node is labelled by the\nconfiguration of an annotated pushdown automaton. This allows the modelling of\nfork and join constructs in higher-order programs and is a generalisation of\nhigher-order stack trees recently introduced by Penelle.\n<br>We show that, given a regular set of annotated stack trees, the set of trees\nthat can reach this set is also regular, and constructible in n-EXPTIME for an\norder-n system, which is optimal. We also show that our construction can be\nextended to allow a global state through which unrelated nodes of the tree may\ncommunicate, provided the number of communications is subject to a fixed bound.\n", "subjects": "Formal Languages and Automata Theory (cs.FL)", "authors": "Matthew Hague, Vincent Penelle,", "date": "2015-02-24 01:36:16.525829"}, 
{"urllink": "http://arxiv.org/abs/1502.05786", "category": "Distributed, Parallel, and Cluster Computing ", "pdflink": "http://arxiv.org/pdf/1502.05786", "title": "\nRandomized Assignment of Jobs to Servers in Heterogeneous Clusters of  Shared Servers for Low Delay", "abstract": "> We consider the job assignment problem in a multi-server system consisting of\n$N$ parallel processor sharing servers, categorized into $M$ ($\\ll N$)\ndifferent types according to their processing capacity or speed. Jobs of random\nsizes arrive at the system according to a Poisson process with rate $N\n\\lambda$. Upon each arrival, a small number of servers from each type is\nsampled uniformly at random. The job is then assigned to one of the sampled\nservers based on a selection rule. We propose two schemes, each corresponding\nto a specific selection rule that aims at reducing the mean sojourn time of\njobs in the system.\n<br>We first show that both methods achieve the maximal stability region. We then\nanalyze the system operating under the proposed schemes as $N \\to \\infty$ which\ncorresponds to the mean field. Our results show that asymptotic independence\namong servers holds even when $M$ is finite and exchangeability holds only\nwithin servers of the same type. We further establish the existence and\nuniqueness of stationary solution of the mean field and show that the tail\ndistribution of server occupancy decays doubly exponentially for each server\ntype. When the estimates of arrival rates are not available, the proposed\nschemes offer simpler alternatives to achieving lower mean sojourn time of\njobs, as shown by our numerical studies.\n", "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC)", "authors": "Arpan Mukhopadhyay, A. Karthik, Ravi R. Mazumdar,", "date": "2015-02-24 01:36:16.292102"}, 
{"urllink": "http://arxiv.org/abs/1410.8027", "category": "General Literature ", "pdflink": "http://arxiv.org/pdf/1410.8027", "title": "\nTowards a Visual Turing Challenge", "abstract": "> As language and visual understanding by machines progresses rapidly, we are\nobserving an increasing interest in holistic architectures that tightly\ninterlink both modalities in a joint learning and inference process. This trend\nhas allowed the community to progress towards more challenging and open tasks\nand refueled the hope at achieving the old AI dream of building machines that\ncould pass a turing test in open domains. In order to steadily make progress\ntowards this goal, we realize that quantifying performance becomes increasingly\ndifficult. Therefore we ask how we can precisely define such challenges and how\nwe can evaluate different algorithms on this open tasks? In this paper, we\nsummarize and discuss such challenges as well as try to give answers where\nappropriate options are available in the literature. We exemplify some of the\nsolutions on a recently presented dataset of question-answering task based on\nreal-world indoor images that establishes a visual turing challenge. Finally,\nwe argue despite the success of unique ground-truth annotation, we likely have\nto step away from carefully curated dataset and rather rely on 'social\nconsensus' as the main driving force to create suitable benchmarks. Providing\ncoverage in this inherently ambiguous output space is an emerging challenge\nthat we face in order to make quantifiable progress in this area.\n", "subjects": "Artificial Intelligence (cs.AI)", "authors": "Mateusz Malinowski, Mario Fritz,", "date": "2015-02-24 01:36:16.761425"}, 
{"urllink": "http://arxiv.org/abs/1502.05825", "category": "Emerging Technologies ", "pdflink": "http://arxiv.org/pdf/1502.05825", "title": "\nSelf-Inverse Functions and Palindromic Circuits", "abstract": "> We investigate the subclass of reversible functions that are self-inverse and\nrelate them to reversible circuits that are equal to their reverse circuit,\nwhich are called palindromic circuits. We precisely determine which\nself-inverse functions can be realized as a palindromic circuit. For those\nfunctions that cannot be realized as a palindromic circuit, we find alternative\npalindromic representations that require an extra circuit line or quantum gates\nin their construction. Our analyses make use of involutions in the symmetric\ngroup $S_{2^n}$ which are isomorphic to self-inverse reversible function on $n$\nvariables.\n", "subjects": "Emerging Technologies (cs.ET)", "authors": "Mathias Soeken, Michael Kirkedal Thomsen, Gerhard W. Dueck, D. Michael Miller,", "date": "2015-02-24 01:36:16.816018"}, 
{"urllink": "http://arxiv.org/abs/1502.05745", "category": "Distributed, Parallel, and Cluster Computing ", "pdflink": "http://arxiv.org/pdf/1502.05745", "title": "\nPolylogarithmic-Time Leader Election in Population Protocols Using  Polylogarithmic States", "abstract": "> Population protocols are networks of finite-state agents, interacting\nrandomly, and updating their states using simple rules. Despite their extreme\nsimplicity, these systems have been shown to cooperatively perform complex\ncomputational tasks, such as simulating register machines to compute standard\narithmetic functions. The election of a unique leader agent is a key\nrequirement in such computational constructions. Yet, the fastest currently\nknown population protocol for electing a leader only has linear convergence\ntime, and, it has recently been shown that no population protocol using a\nconstant number of states per node may overcome this linear bound.\n<br>In this paper, we give the first population protocol for leader election with\npolylogarithmic convergence time, using polylogarithmic memory states per node.\nThe protocol structure is quite simple: each node has an associated value, and\nis either a leader (still in contention) or a minion (following some leader). A\nleader keeps incrementing its value and \"defeats\" other leaders in one-to-one\ninteractions, and will drop from contention and become a minion if it meets a\nleader with higher value. Importantly, a leader also drops out if it meets a\nminion with higher absolute value. While these rules are quite simple, the\nproof that this algorithm achieves polylogarithmic convergence time is\nnon-trivial. In particular, the argument combines careful use of concentration\ninequalities with anti-concentration bounds, showing that the leaders' values\nbecome spread apart as the execution progresses, which in turn implies that\nstraggling leaders get quickly eliminated. We complement our analysis with\nempirical results, showing that our protocol converges extremely fast, even for\nlarge network sizes.\n", "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC)", "authors": "Dan Alistarh, Rati Gelashvili,", "date": "2015-02-24 01:36:16.291336"}, 
{"urllink": "http://arxiv.org/abs/1502.05730", "category": "Distributed, Parallel, and Cluster Computing ", "pdflink": "http://arxiv.org/pdf/1502.05730", "title": "\nDesigning Applications with Distributed Databases in a Hybrid Cloud", "abstract": "> Designing applications for use in a hybrid cloud has many features. These\ninclude dynamic virtualization management and an unknown route switching\ncustomers. This makes it impossible to evaluate the query and hence the optimal\ndistribution of data. In this paper, we formulate the main challenges of\ndesigning and simulation offer installation for processing.\n", "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC)", "authors": "Evgeniy Pluzhnik, Oleg Lukyanchikov, Evgeny Nikulchev, Simon Payain,", "date": "2015-02-24 01:36:16.290499"}, 
{"urllink": "http://arxiv.org/abs/1502.05676", "category": "Digital Libraries ", "pdflink": "http://arxiv.org/pdf/1502.05676", "title": "\nJournal Portfolio Analysis for Countries, Cities, and Organizations:  Maps and Comparisons", "abstract": "> Using Web-of-Science data, portfolio analysis in terms of journal coverage\ncan be projected on a base map for units of analysis such as countries, cities,\nuniversities, and firms. The units of analysis under study can be compared\nstatistically across the 10,000+ journals. The interdisciplinarity of the\nportfolios can be measured using Rao-Stirling diversity or Zhang et al.'s (in\npress) improved measure 2D3. At the country level we find regional\ndifferentiation (e.g., Latin-American or Asian countries), but also a major\ndivide between advanced and less-developed countries. Israel and Israeli cities\noutperform other nations and cities in terms of diversity. Universities appear\nto be specifically related to firms when a number of these units are\nexploratively compared. The instrument is relatively simple and\nstraightforward, and one can generalize the application to any document set\nretrieved from WoS. Further instruction is provided online at\n<a href=\"http://www.leydesdorff.net/portfolio.\">this http URL</a>\n", "subjects": "Digital Libraries (cs.DL)", "authors": "Loet Leydesdorff, Gaston Heimeriks, Daniele Rotolo,", "date": "2015-02-24 01:36:16.190952"}, 
{"urllink": "http://arxiv.org/abs/1502.05811", "category": "Discrete Mathematics ", "pdflink": "http://arxiv.org/pdf/1502.05811", "title": "\nRotor-routing orbits in directed graphs and the Picard group", "abstract": "> In [5], Holroyd, Levine, M\\'esz\\'aros, Peres, Propp and Wilson characterize\nrecurrent chip-and-rotor configurations for strongly connected digraphs.\nHowever, the number of steps needed to recur, and the number of orbits is left\nopen for general digraphs. Recently, these questions were ans\\-wered by Pham\n[6], using linear algebraic methods. We give new, purely combinatorial proofs\nfor these formulas. We also relate rotor-router orbits to the chip-firing game:\nThe number of recurrent rotor-router unicycle-orbits equals the order of the\nPicard group of the graph, defined in the sense of [1], and during a period,\nthe same chip-moves happen, as during firing the period vector in the\nchip-firing game.\n", "subjects": "Discrete Mathematics (cs.DM)", "authors": "Lilla T\u00f3thm\u00e9r\u00e9sz,", "date": "2015-02-24 01:36:16.167384"}, 
{"urllink": "http://arxiv.org/abs/1502.05943", "category": "Databases ", "pdflink": "http://arxiv.org/pdf/1502.05943", "title": "\nRefining Adverse Drug Reactions using Association Rule Mining for  Electronic Healthcare Data", "abstract": "> Side effects of prescribed medications are a common occurrence. Electronic\nhealthcare databases present the opportunity to identify new side effects\nefficiently but currently the methods are limited due to confounding (i.e. when\nan association between two variables is identified due to them both being\nassociated to a third variable).\n<br>In this paper we propose a proof of concept method that learns common\nassociations and uses this knowledge to automatically refine side effect\nsignals (i.e. exposure-outcome associations) by removing instances of the\nexposure-outcome associations that are caused by confounding. This leaves the\nsignal instances that are most likely to correspond to true side effect\noccurrences. We then calculate a novel measure termed the confounding-adjusted\nrisk value, a more accurate absolute risk value of a patient experiencing the\noutcome within 60 days of the exposure.\n<br>Tentative results suggest that the method works. For the four signals (i.e.\nexposure-outcome associations) investigated we are able to correctly filter the\nmajority of exposure-outcome instances that were unlikely to correspond to true\nside effects. The method is likely to improve when tuning the association rule\nmining parameters for specific health outcomes.\n<br>This paper shows that it may be possible to filter signals at a patient level\nbased on association rules learned from considering patients' medical\nhistories. However, additional work is required to develop a way to automate\nthe tuning of the method's parameters.\n", "subjects": "Databases (cs.DB)", "authors": "Jenna M. Reps, Uwe Aickelin, Jiangang Ma, Yanchun Zhang,", "date": "2015-02-24 01:36:15.882328"}, 
{"urllink": "http://arxiv.org/abs/1502.05910", "category": "Data Structures and Algorithms ", "pdflink": "http://arxiv.org/pdf/1502.05910", "title": "\nFixed-parameter Tractable Distances to Sparse Graph Classes", "abstract": "> We show that for various classes C of sparse graphs, and several measures of\ndistance to such classes (such as edit distance and elimination distance), the\nproblem of determining the distance of a given graph G to C is fixed-parameter\ntractable. The results are based on two general techniques. The first of these,\nbuilding on recent work of Grohe et al. establishes that any class of graphs\nthat is slicewise nowhere dense and slicewise first-order definable is FPT. The\nsecond shows that determining the elimination distance of a graph G to a\nminor-closed class C is FPT.\n", "subjects": "Data Structures and Algorithms (cs.DS)", "authors": "Jannis Bulian, Anuj Dawar,", "date": "2015-02-24 01:36:15.788200"}, 
{"urllink": "http://arxiv.org/abs/1502.05844", "category": "Databases ", "pdflink": "http://arxiv.org/pdf/1502.05844", "title": "\nAn Approach For Transforming of Relational Databases to OWL Ontology", "abstract": "> Rapid growth of documents, web pages, and other types of text content is a\nhuge challenge for the modern content management systems. One of the problems\nin the areas of information storage and retrieval is the lacking of semantic\ndata. Ontologies can present knowledge in sharable and repeatedly usable manner\nand provide an effective way to reduce the data volume overhead by encoding the\nstructure of a particular domain. Metadata in relational databases can be used\nto extract ontology from database in a special domain. According to solve the\nproblem of sharing and reusing of data, approaches based on transforming\nrelational database to ontology are proposed. In this paper we propose a method\nfor automatic ontology construction based on relational database. Mining and\nobtaining further components from relational database leads to obtain knowledge\nwith high semantic power and more expressiveness. Triggers are one of the\ndatabase components which could be transformed to the ontology model and\nincrease the amount of power and expressiveness of knowledge by presenting part\nof the knowledge dynamically\n", "subjects": "Databases (cs.DB)", "authors": "Mona Dadjoo, Esmaeil Kheirkhah,", "date": "2015-02-24 01:36:15.881614"}, 
{"urllink": "http://arxiv.org/abs/1502.05828", "category": "Data Structures and Algorithms ", "pdflink": "http://arxiv.org/pdf/1502.05828", "title": "\nTime-Approximation Trade-offs for Inapproximable Problems", "abstract": "> In this paper we focus on problems which do not admit a constant-factor\napproximation in polynomial time and explore how quickly their approximability\nimproves as the allowed running time is gradually increased from polynomial to\n(sub-)exponential.\n<br>We tackle a number of problems: For Min Independent Dominating Set, Max\nInduced Path, Forest and Tree, for any $r(n)$, a simple, known scheme gives an\napproximation ratio of $r$ in time roughly $r^{n/r}$. We show that, for most\nvalues of $r$, if this running time could be significantly improved the ETH\nwould fail. For Max Minimal Vertex Cover we give a non-trivial\n$\\sqrt{r}$-approximation in time $2^{n/r}$. We match this with a similarly\ntight result. We also give a $\\log r$-approximation for Min ATSP in time\n$2^{n/r}$ and an $r$-approximation for Max Grundy Coloring in time $r^{n/r}$.\n<br>Furthermore, we show that Min Set Cover exhibits a curious behavior in this\nsuper-polynomial setting: for any $\\delta &gt; 0$ it admits an\n$m^\\delta$-approximation, where $m$ is the number of sets, in just\nquasi-polynomial time. We observe that if such ratios could be achieved in\npolynomial time, the ETH or the Projection Games Conjecture would fail.\n", "subjects": "Data Structures and Algorithms (cs.DS)", "authors": "\u00c9douard Bonnet, Michael Lampis, Vangelis Th. Paschos,", "date": "2015-02-24 01:36:15.787214"}, 
{"urllink": "http://arxiv.org/abs/1502.05746", "category": "Data Structures and Algorithms ", "pdflink": "http://arxiv.org/pdf/1502.05746", "title": "\nBinary Embedding: Fundamental Limits and Fast Algorithm", "abstract": "> Binary embedding is a nonlinear dimension reduction methodology where high\ndimensional data are embedded into the Hamming cube while preserving the\nstructure of the original space. Specifically, for an arbitrary $N$ distinct\npoints in $\\mathbb{S}^{p-1}$, our goal is to encode each point using\n$m$-dimensional binary strings such that we can reconstruct their geodesic\ndistance up to $\\delta$ uniform distortion. Existing binary embedding\nalgorithms either lack theoretical guarantees or suffer from running time\n$O\\big(mp\\big)$. We make three contributions: (1) we establish a lower bound\nthat shows any binary embedding oblivious to the set of points requires $m =\n\\Omega(\\frac{1}{\\delta^2}\\log{N})$ bits and a similar lower bound for\nnon-oblivious embeddings into Hamming distance; (2) we propose a novel fast\nbinary embedding algorithm with provably optimal bit complexity $m =\nO\\big(\\frac{1}{\\delta^2}\\log{N}\\big)$ and near linear running time $O(p \\log\np)$ whenever $\\log N \\ll \\delta \\sqrt{p}$, with a slightly worse running time\nfor larger $\\log N$; (3) we also provide an analytic result about embedding a\ngeneral set of points $K \\subseteq \\mathbb{S}^{p-1}$ with even infinite size.\nOur theoretical findings are supported through experiments on both synthetic\nand real data sets.\n", "subjects": "Data Structures and Algorithms (cs.DS)", "authors": "Xinyang Yi, Constantine Caramanis, Eric Price,", "date": "2015-02-24 01:36:15.786199"}, 
{"urllink": "http://arxiv.org/abs/1502.05729", "category": "Data Structures and Algorithms ", "pdflink": "http://arxiv.org/pdf/1502.05729", "title": "\nQuicksort, Largest Bucket, and Min-Wise Hashing with Limited  Independence", "abstract": "> Randomized algorithms and data structures are often analyzed under the\nassumption of access to a perfect source of randomness. The most fundamental\nmetric used to measure how \"random\" a hash function or a random number\ngenerator is, is its independence: a sequence of random variables is said to be\n$k$-independent if every variable is uniform and every size $k$ subset is\nindependent. In this paper we consider three classic algorithms under limited\nindependence. We provide new bounds for randomized quicksort, min-wise hashing\nand largest bucket size under limited independence. Our results can be\nsummarized as follows.\n<br>-Randomized quicksort. When pivot elements are computed using a\n$5$-independent hash function, Karloff and Raghavan, J.ACM'93 showed $O ( n\n\\log n)$ expected worst-case running time for a special version of quicksort.\nWe improve upon this, showing that the same running time is achieved with only\n$4$-independence.\n<br>-Min-wise hashing. For a set $A$, consider the probability of a particular\nelement being mapped to the smallest hash value. It is known that\n$5$-independence implies the optimal probability $O (1 /n)$. Broder et al.,\nSTOC'98 showed that $2$-independence implies it is $O(1 / \\sqrt{|A|})$. We show\na matching lower bound as well as new tight bounds for $3$- and $4$-independent\nhash functions.\n<br>-Largest bucket. We consider the case where $n$ balls are distributed to $n$\nbuckets using a $k$-independent hash function and analyze the largest bucket\nsize. Alon et. al, STOC'97 showed that there exists a $2$-independent hash\nfunction implying a bucket of size $\\Omega ( n^{1/2})$. We generalize the\nbound, providing a $k$-independent family of functions that imply size $\\Omega\n( n^{1/k})$.\n", "subjects": "Data Structures and Algorithms (cs.DS)", "authors": "Mathias B\u00e6k Tejs Knudsen, Morten St\u00f6ckel,", "date": "2015-02-24 01:36:15.784628"}, 
{"urllink": "http://arxiv.org/abs/1502.05908", "category": "Computer Vision and Pattern Recognition ", "pdflink": "http://arxiv.org/pdf/1502.05908", "title": "\nLearning Descriptors for Object Recognition and 3D Pose Estimation", "abstract": "> Detecting poorly textured objects and estimating their 3D pose reliably is\nstill a very challenging problem. We introduce a simple but powerful approach\nto computing descriptors for object views that efficiently capture both the\nobject identity and 3D pose. By contrast with previous manifold-based\napproaches, we can rely on the Euclidean distance to evaluate the similarity\nbetween descriptors, and therefore use scalable Nearest Neighbor search methods\nto efficiently handle a large number of objects under a large range of poses.\nTo achieve this, we train a Convolutional Neural Network to compute these\ndescriptors by enforcing simple similarity and dissimilarity constraints\nbetween the descriptors. We show that our constraints nicely untangle the\nimages from different objects and different views into clusters that are not\nonly well-separated but also structured as the corresponding sets of poses: The\nEuclidean distance between descriptors is large when the descriptors are from\ndifferent objects, and directly related to the distance between the poses when\nthe descriptors are from the same object. These important properties allow us\nto outperform state-of-the-art object views representations on challenging RGB\nand RGB-D data.\n", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "authors": "Paul Wohlhart, Vincent Lepetit,", "date": "2015-02-24 01:36:15.544827"}, 
{"urllink": "http://arxiv.org/abs/1502.05840", "category": "Computer Vision and Pattern Recognition ", "pdflink": "http://arxiv.org/pdf/1502.05840", "title": "\nA General Multi-Graph Matching Approach via Graduated  Consistency-regularized Boosting", "abstract": "> This paper addresses the problem of matching $N$ weighted graphs referring to\nan identical object or category. More specifically, matching the common node\ncorrespondences among graphs. This multi-graph matching problem involves two\ningredients affecting the overall accuracy: i) the local pairwise matching\naffinity score among graphs; ii) the global matching consistency that measures\nthe uniqueness of the pairwise matching results by different chaining orders.\nPrevious studies typically either enforce the matching consistency constraints\nin the beginning of iterative optimization, which may propagate matching error\nboth over iterations and across graph pairs; or separate affinity optimizing\nand consistency regularization in two steps. This paper is motivated by the\nobservation that matching consistency can serve as a regularizer in the\naffinity objective function when the function is biased due to noises or\ninappropriate modeling. We propose multi-graph matching methods to incorporate\nthe two aspects by boosting the affinity score, meanwhile gradually infusing\nthe consistency as a regularizer. Furthermore, we propose a node-wise\nconsistency/affinity-driven mechanism to elicit the common inlier nodes out of\nthe irrelevant outliers. Extensive results on both synthetic and public image\ndatasets demonstrate the competency of the proposed algorithms.\n", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "authors": "Junchi Yan, Minsu Cho, Hongyuan Zha, Xiaokang Yang, Stephen Chu,", "date": "2015-02-24 01:36:15.544208"}, 
{"urllink": "http://arxiv.org/abs/1502.05803", "category": "Computer Vision and Pattern Recognition ", "pdflink": "http://arxiv.org/pdf/1502.05803", "title": "\nVisual object tracking performance measures revisited", "abstract": "> The problem of visual tracking evaluation is sporting a large variety of\nperformance measures, and largely suffers from lack of consensus about which\nmeasures should be used in experiments. This makes the cross-paper tracker\ncomparison difficult. Furthermore, as some measures may be less effective than\nothers, the tracking results may be skewed or biased towards particular\ntracking aspects. In this paper we revisit the popular performance measures and\ntracker performance visualizations and analyze them theoretically and\nexperimentally. We show that several measures are equivalent from the point of\ninformation they provide for tracker comparison and, crucially, that some are\nmore brittle than the others. Based on our analysis we narrow down the set of\npotential measures to only two complementary ones, describing accuracy and\nrobustness, thus pushing towards homogenization of the tracker evaluation\nmethodology. These two measures can be intuitively interpreted and visualized\nand have been employed by the recent Visual Object Tracking (VOT) challenges as\nthe foundation for the evaluation methodology.\n", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "authors": "Luka \u010cehovin, Ale\u0161 Leonardis, Matej Kristan,", "date": "2015-02-24 01:36:15.543435"}, 
{"urllink": "http://arxiv.org/abs/1502.05752", "category": "Computer Vision and Pattern Recognition ", "pdflink": "http://arxiv.org/pdf/1502.05752", "title": "\nPairwise Constraint Propagation: A Survey", "abstract": "> As one of the most important types of (weaker) supervised information in\nmachine learning and pattern recognition, pairwise constraint, which specifies\nwhether a pair of data points occur together, has recently received significant\nattention, especially the problem of pairwise constraint propagation. At least\ntwo reasons account for this trend: the first is that compared to the data\nlabel, pairwise constraints are more general and easily to collect, and the\nsecond is that since the available pairwise constraints are usually limited,\nthe constraint propagation problem is thus important.\n<br>This paper provides an up-to-date critical survey of pairwise constraint\npropagation research. There are two underlying motivations for us to write this\nsurvey paper: the first is to provide an up-to-date review of the existing\nliterature, and the second is to offer some insights into the studies of\npairwise constraint propagation. To provide a comprehensive survey, we not only\ncategorize existing propagation techniques but also present detailed\ndescriptions of representative methods within each category.\n", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "authors": "Zhenyong Fu, Zhiwu Lu,", "date": "2015-02-24 01:36:15.542822"}, 
{"urllink": "http://arxiv.org/abs/1502.05186", "category": "Computers and Society ", "pdflink": "http://arxiv.org/pdf/1502.05186", "title": "\nMeasuring Accuracy of Automated Parsing and Categorization Tools and  Processes in Digital Investigations", "abstract": "> This work presents a method for the measurement of the accuracy of evidential\nartifact extraction and categorization tasks in digital forensic\ninvestigations. Instead of focusing on the measurement of accuracy and errors\nin the functions of digital forensic tools, this work proposes the application\nof information retrieval measurement techniques that allow the incorporation of\nerrors introduced by tools and analysis processes. This method uses a `gold\nstandard' that is the collection of evidential objects determined by a digital\ninvestigator from suspect data with an unknown ground truth. This work proposes\nthat the accuracy of tools and investigation processes can be evaluated\ncompared to the derived gold standard using common precision and recall values.\nTwo example case studies are presented showing the measurement of the accuracy\nof automated analysis tools as compared to an in-depth analysis by an expert.\nIt is shown that such measurement can allow investigators to determine changes\nin accuracy of their processes over time, and determine if such a change is\ncaused by their tools or knowledge.\n", "subjects": "Computers and Society (cs.CY)", "authors": "Joshua I. James, Alejandra Lopez-Fernandez, Pavel Gladyshev,", "date": "2015-02-24 01:36:15.514649"}, 
{"urllink": "http://arxiv.org/abs/1502.05742", "category": "Computer Vision and Pattern Recognition ", "pdflink": "http://arxiv.org/pdf/1502.05742", "title": "\nApplication of Independent Component Analysis Techniques in Speckle  Noise Reduction of Single-Shot Retinal OCT Images", "abstract": "> Optical Coherence Tomography (OCT) is an emerging technique in the field of\nbiomedical imaging, with applications in ophthalmology, dermatology, coronary\nimaging etc. OCT images usually suffer from a granular pattern, called speckle\nnoise, which restricts the process of interpretation. To the best of knowledge,\nuse of Independent Component Analysis (ICA) techniques has never been explored\nfor speckle reduction of OCT images. Here, a comparative study of several ICA\ntechniques is provided for noise reduction of single-shot retinal OCT images.\nHaving multiple B-scans of the same location, the eye movements are compensated\nusing a rigid registration technique. Then, different ICA techniques are\napplied to the aggregated set of B-scans for extracting the noise-free image.\nSignal-to-Noise-Ratio (SNR), Contrast-to-Noise-Ratio (CNR) and\nEquivalent-Number-of-Looks (ENL), as well as analysis on the computational\ncomplexity of the methods, are considered as metrics for comparison. The\nresults show that use of ICA can be beneficial, especially in case of having\nfewer number of B-scans.\n", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "authors": "Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu,", "date": "2015-02-24 01:36:15.542065"}, 
{"urllink": "http://arxiv.org/abs/1502.05145", "category": "Computers and Society ", "pdflink": "http://arxiv.org/pdf/1502.05145", "title": "\nKnowledge-generating Efficiency in Innovation Systems: The relation  between structural and temporal effects", "abstract": "> Using time series of US patents per million inhabitants, knowledge-generating\ncycles can be distinguished. These cycles partly coincide with Kondratieff long\nwaves. The changes in the slopes between them indicate discontinuities in the\nknowledge-generating paradigms. The knowledge-generating paradigms can be\nmodeled in terms of interacting dimensions (for example, in\nuniversity-industry-government relations) that set limits to the maximal\nefficiency of innovation systems. The maximum values of the parameters in the\nmodel are of the same order as the regression coefficients of the empirical\nwaves. The mechanism of the increase in the dimensionality is specified as\nself-organization which leads to the breaking of existing relations into the\nmore diversified structure of a fractal-like network. This breaking can be\nmodeled in analogy to 2D and 3D (Koch) snowflakes. The boost of knowledge\ngeneration leads to newly emerging technologies that can be expected to be more\ndiversified and show shorter life cycles than before. Time spans of the\nknowledge-generating cycles can also be analyzed in terms of Fibonacci numbers.\nThis perspective allows for forecasting expected dates of future possible\nparadigm changes. In terms of policy implications, this suggests a shift in\nfocus from the manufacturing technologies to developing new organizational\ntechnologies and formats of human interactions\n", "subjects": "Computers and Society (cs.CY)", "authors": "Inga Ivanova, Loet Leydesdorff,", "date": "2015-02-24 01:36:15.513950"}, 
{"urllink": "http://arxiv.org/abs/1502.05774", "category": "Computer Science and Game Theory ", "pdflink": "http://arxiv.org/pdf/1502.05774", "title": "\nActively Purchasing Data for Learning", "abstract": "> We design mechanisms for online procurement of data held by strategic agents\nfor machine learning tasks. The challenge is to use past data to actively price\nfuture data and give learning guarantees even when an agent's cost for\nrevealing her data may depend arbitrarily on the data itself. We achieve this\ngoal by showing how to convert a large class of no-regret algorithms into\nonline posted-price and learning mechanisms. Our results in a sense parallel\nclassic sample complexity guarantees, but with the key resource being money\nrather than quantity of data: With a budget constraint $B$, we give robust risk\n(predictive error) bounds on the order of $1/\\sqrt{B}$. Because we use an\nactive approach, we can often guarantee to do significantly better by\nleveraging correlations between costs and data.\n<br>Our algorithms and analysis go through a model of no-regret learning with $T$\narriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds\nfor this model are on the order of $T/\\sqrt{B}$ and we give lower bounds on the\nsame order.\n", "subjects": "Computer Science and Game Theory (cs.GT)", "authors": "Jacob Abernethy, Yiling Chen, Chien-Ju Ho, Bo Waggoner,", "date": "2015-02-24 01:36:15.250818"}, 
{"urllink": "http://arxiv.org/abs/1502.05838", "category": "Artificial Intelligence ", "pdflink": "http://arxiv.org/pdf/1502.05838", "title": "\nAutomated Reasoning for Robot Ethics", "abstract": "> Deontic logic is a very well researched branch of mathematical logic and\nphilosophy. Various kinds of deontic logics are considered for different\napplication domains like argumentation theory, legal reasoning, and acts in\nmulti-agent systems. In this paper, we show how standard deontic logic can be\nused to model ethical codes for multi-agent systems. Furthermore we show how\nHyper, a high performance theorem prover, can be used to prove properties of\nthese ethical codes.\n", "subjects": "Artificial Intelligence (cs.AI)", "authors": "Ulrich Furbach, Claudia Schon, Frieder Stolzenburg,", "date": "2015-02-24 01:36:14.964118"}, 
{"urllink": "http://arxiv.org/abs/1502.05864", "category": "Artificial Intelligence ", "pdflink": "http://arxiv.org/pdf/1502.05864", "title": "\nPseudo Fuzzy Set", "abstract": "> Here a novel idea to handle imprecise or vague set viz. Pseudo fuzzy set has\nbeen proposed. Pseudo fuzzy set is a triplet of element and its two membership\nfunctions. Both the membership functions may or may not be dependent. The\nhypothesis is that every positive sense has some negative sense. So, one\nmembership function has been considered as positive and another as negative.\nConsidering this concept, here the development of Pseudo fuzzy set and its\nproperty along with Pseudo fuzzy numbers has been discussed.\n", "subjects": "Artificial Intelligence (cs.AI)", "authors": "Sukanta Nayak, Snehashish Chakraverty,", "date": "2015-02-24 01:36:14.965098"}, 
{"urllink": "http://arxiv.org/abs/1502.04268", "category": "Graphics ", "pdflink": "http://arxiv.org/pdf/1502.04268", "title": "\nRelative Squared Distances to a Conic Berserkless 8-Connected Midpoint  Algorithm", "abstract": "> The midpoint method or technique is a measurement and as each measurement it\nhas a tolerance, but worst of all it can be invalid, called Out-of-Control or\nOoC. The core of all midpoint methods is the accurate measurement of the\ndifference of the squared distances of two points to the polar of their\nmidpoint with respect to the conic. When this measurement is valid, it also\nmeasures the difference of the squared distances of these points to the conic,\nalthough it may be inaccurate, called Out-of-Accuracy or OoA. The primary\ncondition is the necessary and sufficient condition that a measurement is\nvalid. It is comletely new and it can be checked ultra fast and before the\nactual measurement starts. Modeling an incremental algorithm, shows that the\ncurve must be subdivided into piecewise monotonic sections, the start point\nmust be optimal, and it explains that the 2D-incremental method can find,\nlocally, the global Least Square Distance. Locally means that there are at most\nthree candidate points for a given monotonic direction; therefore the\n2D-midpoint method has, locally, at most three measurements. When all the\npossible measurements are invalid, the midpoint method cannot be applied, and\nin that case the ultra fast OoC-rule selects the candidate point. This\nguarantees, for the first time, a 100% stable, ultra-fast, berserkless midpoint\nalgorithm, which can be easily transformed to hardware.\n", "subjects": "Graphics (cs.GR)", "authors": "Valere Huypens,", "date": "2015-02-24 01:36:21.753281"}, 
{"urllink": "http://arxiv.org/abs/1502.04221", "category": "Hardware Architecture ", "pdflink": "http://arxiv.org/pdf/1502.04221", "title": "\nA Row-parallel 8$\\times$8 2-D DCT Architecture Using Algebraic Integer  Based Exact Computation", "abstract": "> An algebraic integer (AI) based time-multiplexed row-parallel architecture\nand two final-reconstruction step (FRS) algorithms are proposed for the\nimplementation of bivariate AI-encoded 2-D discrete cosine transform (DCT). The\narchitecture directly realizes an error-free 2-D DCT without using FRSs between\nrow-column transforms, leading to an 8$\\times$8 2-D DCT which is entirely free\nof quantization errors in AI basis. As a result, the user-selectable accuracy\nfor each of the coefficients in the FRS facilitates each of the 64 coefficients\nto have its precision set independently of others, avoiding the leakage of\nquantization noise between channels as is the case for published DCT designs.\nThe proposed FRS uses two approaches based on (i) optimized Dempster-Macleod\nmultipliers and (ii) expansion factor scaling. This architecture enables\nlow-noise high-dynamic range applications in digital video processing that\nrequires full control of the finite-precision computation of the 2-D DCT. The\nproposed architectures and FRS techniques are experimentally verified and\nvalidated using hardware implementations that are physically realized and\nverified on FPGA chip. Six designs, for 4- and 8-bit input word sizes, using\nthe two proposed FRS schemes, have been designed, simulated, physically\nimplemented and measured. The maximum clock rate and block-rate achieved among\n8-bit input designs are 307.787 MHz and 38.47 MHz, respectively, implying a\npixel rate of 8$\\times$307.787$\\approx$2.462 GHz if eventually embedded in a\nreal-time video-processing system. The equivalent frame rate is about 1187.35\nHz for the image size of 1920$\\times$1080. All implementations are functional\non a Xilinx Virtex-6 XC6VLX240T FPGA device.\n", "subjects": "Hardware Architecture (cs.AR)", "authors": "A. Madanayake, R. J. Cintra, D. Onen, V. S. Dimitrov, N. T. Rajapaksha, L. T. Bruton, A. Edirisuriya,", "date": "2015-02-24 01:36:21.758074"}, 
{"urllink": "http://arxiv.org/abs/1502.05534", "category": "Human-Computer Interaction ", "pdflink": "http://arxiv.org/pdf/1502.05534", "title": "\nNeuroSVM: A Graphical User Interface for Identification of Liver  Patients", "abstract": "> Diagnosis of liver infection at preliminary stage is important for better\ntreatment. In todays scenario devices like sensors are used for detection of\ninfections. Accurate classification techniques are required for automatic\nidentification of disease samples. In this context, this study utilizes data\nmining approaches for classification of liver patients from healthy\nindividuals. Four algorithms (Naive Bayes, Bagging, Random forest and SVM) were\nimplemented for classification using R platform. Further to improve the\naccuracy of classification a hybrid NeuroSVM model was developed using SVM and\nfeed-forward artificial neural network (ANN). The hybrid model was tested for\nits performance using statistical parameters like root mean square error (RMSE)\nand mean absolute percentage error (MAPE). The model resulted in a prediction\naccuracy of 98.83%. The results suggested that development of hybrid model\nimproved the accuracy of prediction. To serve the medicinal community for\nprediction of liver disease among patients, a graphical user interface (GUI)\nhas been developed using R. The GUI is deployed as a package in local\nrepository of R platform for users to perform prediction.\n", "subjects": "Learning (cs.LG)", "authors": "Kalyan Nagaraj, Amulyashree Sridhar,", "date": "2015-02-24 01:36:22.084844"}, 
{"urllink": "http://arxiv.org/abs/1502.05980", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05980", "title": "\nCompressive Sensing Reconstruction for Sparse 2D Data", "abstract": "> In this paper we study the compressive sensing effects on 2D signals\nexhibiting sparsity in 2D DFT domain. A simple algorithm for reconstruction of\nrandomly under-sampled data is proposed. It is based on the analytically\ndetermined threshold that precisely separates signal and non-signal components\nin the 2D DFT domain. The proposed solution shows promising results in ISAR\nimaging, where the reconstruction is achieved even in the case when less than\n10% of data is available.\n", "subjects": "Information Theory (cs.IT)", "authors": "Srdjan Stankovic, Irena Orovic,", "date": "2015-02-24 01:36:22.104288"}, 
{"urllink": "http://arxiv.org/abs/1502.05988", "category": "Learning ", "pdflink": "http://arxiv.org/pdf/1502.05988", "title": "\nDeep Learning for Multi-label Classification", "abstract": "> In multi-label classification, the main focus has been to develop ways of\nlearning the underlying dependencies between labels, and to take advantage of\nthis at classification time. Developing better feature-space representations\nhas been predominantly employed to reduce complexity, e.g., by eliminating\nnon-helpful feature attributes from the input space prior to (or during)\ntraining. This is an important task, since many multi-label methods typically\ncreate many different copies or views of the same input data as they transform\nit, and considerable memory can be saved by taking advantage of redundancy. In\nthis paper, we show that a proper development of the feature space can make\nlabels less interdependent and easier to model and predict at inference time.\nFor this task we use a deep learning approach with restricted Boltzmann\nmachines. We present a deep network that, in an empirical evaluation,\noutperforms a number of competitive methods from the literature\n", "subjects": "Learning (cs.LG)", "authors": "Jesse Read, Fernando Perez-Cruz,", "date": "2015-02-24 01:36:22.257669"}, 
{"urllink": "http://arxiv.org/abs/1502.05934", "category": "Learning ", "pdflink": "http://arxiv.org/pdf/1502.05934", "title": "\nAchieving All with No Parameters: Adaptive NormalHedge", "abstract": "> We study the classic online learning problem of predicting with expert\nadvice, and propose a truly parameter-free and adaptive algorithm that achieves\nseveral objectives simultaneously without using any prior information. The main\ncomponent of this work is an improved version of the NormalHedge.DT algorithm\n(Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this new\nalgorithm ensures small regret when the competitor has small loss and almost\nconstant regret when the losses are stochastic. On the other hand, the\nalgorithm is able to compete with any convex combination of the experts\nsimultaneously, with a regret in terms of the relative entropy of the prior and\nthe competitor. This resolves an open problem proposed by Chaudhuri et al.\n(2009) and Chernov and Vovk (2010). Moreover, we extend the results to the\nsleeping expert setting and provide two applications to illustrate the power of\nAdaNormalHedge: 1) competing with time-varying unknown competitors and 2)\npredicting almost as well as the best pruning tree. Our results on these\napplications significantly improve previous work from different aspects, and a\nspecial case of the first application resolves another open problem proposed by\nWarmuth and Koolen (2014) on whether one can simultaneously achieve optimal\nshifting regret for both adversarial and stochastic losses.\n", "subjects": "Learning (cs.LG)", "authors": "Haipeng Luo, Robert E. Schapire,", "date": "2015-02-24 01:36:22.256700"}, 
{"urllink": "http://arxiv.org/abs/1502.05955", "category": "Information Retrieval ", "pdflink": "http://arxiv.org/pdf/1502.05955", "title": "\nStream Sampling for Frequency Cap Statistics", "abstract": "> Unaggregated data streams are prevalent and come from diverse application\ndomains which include interactions of users with web services and IP traffic.\nThe elements of the stream have {\\em keys} (cookies, users, queries) and\nelements with different keys interleave in the stream. Analytics on such data\ntypically utilizes statistics stated in terms of the frequencies of keys. The\ntwo most common statistics are {\\em distinct keys}, which is the number of\nactive keys in a specified segment, and {\\em sum}, which is the sum of the\nfrequencies of keys in the segment. These are two special cases of {\\em\nfrequency cap} statistics, defined as the sum of frequencies {\\em capped} by a\nparameter $T$, which are popular in online advertising platforms.\n<br>We propose a novel general framework for sampling unaggregated streams which\nprovides the first effective stream sampling solution for general frequency cap\nstatistics. Our $\\ell$-capped samples provide estimates with tight statistical\nguarantees for cap statistics with $T=\\Theta(\\ell)$ and nonnegative unbiased\nestimates of {\\em any} monotone non-decreasing frequency statistics. Our\nalgorithms and estimators are simple and practical and we demonstrate their\neffectiveness using extensive simulations. An added benefit of our unified\ndesign is facilitating {\\em multi-objective samples}, which provide estimates\nwith statistical guarantees for a specified set of different statistics, using\na single, smaller sample.\n", "subjects": "Information Retrieval (cs.IR)", "authors": "Edith Cohen,", "date": "2015-02-24 01:36:22.060534"}, 
{"urllink": "http://arxiv.org/abs/1502.05860", "category": "Logic in Computer Science ", "pdflink": "http://arxiv.org/pdf/1502.05860", "title": "\nOn the relative proof complexity of deep inference via atomic flows", "abstract": "> We consider the proof complexity of the minimal complete fragment, KS, of\nstandard deep inference systems for propositional logic. To examine the size of\nproofs we employ atomic flows, diagrams that trace structural changes through a\nproof but ignore logical information. As results we obtain a polynomial\nsimulation of versions of Resolution, along with some extensions. We also show\nthat these systems, as well as bounded-depth Frege systems, cannot polynomially\nsimulate KS, by giving polynomial-size proofs of certain variants of the\npropositional pigeonhole principle in KS.\n", "subjects": "Logic in Computer Science (cs.LO)", "authors": "Anupam Das,", "date": "2015-02-24 01:36:22.527842"}, 
{"urllink": "http://arxiv.org/abs/1502.05696", "category": "Multiagent Systems ", "pdflink": "http://arxiv.org/pdf/1502.05696", "title": "\nApproval Voting and Incentives in Crowdsourcing", "abstract": "> The growing need for labeled training data has made crowdsourcing an\nimportant part of machine learning. The quality of crowdsourced labels is,\nhowever, adversely affected by three factors: (1) the workers are not experts;\n(2) the incentives of the workers are not aligned with those of the requesters;\nand (3) the interface does not allow workers to convey their knowledge\naccurately, by forcing them to make a single choice among a set of options. In\nthis paper, we address these issues by introducing approval voting to utilize\nthe expertise of workers who have partial knowledge of the true answer, and\ncoupling it with a (\"strictly proper\") incentive-compatible compensation\nmechanism. We show rigorous theoretical guarantees of optimality of our\nmechanism together with a simple axiomatic characterization. We also conduct\nempirical studies on Amazon Mechanical Turk which validate our approach.\n", "subjects": "Computer Science and Game Theory (cs.GT)", "authors": "Nihar B. Shah, Dengyong Zhou, Yuval Peres,", "date": "2015-02-24 01:36:22.540839"}, 
{"urllink": "http://arxiv.org/abs/1502.05751", "category": "Multimedia ", "pdflink": "http://arxiv.org/pdf/1502.05751", "title": "\nEfficient Synthesis of Room Acoustics via Scattering Delay Networks", "abstract": "> An acoustic reverberator consisting of a network of delay lines connected via\nscattering junctions is proposed. All parameters of the reverberator are\nderived from physical properties of the enclosure it simulates. It allows for\nsimulation of unequal and frequency-dependent wall absorption, as well as\ndirectional sources and microphones. The reverberator renders the first-order\nreflections exactly, while making progressively coarser approximations of\nhigher-order reflections. The rate of energy decay is close to that obtained\nwith the image method (IM) and consistent with the predictions of Sabine and\nEyring equations. The time evolution of the normalized echo density, which was\npreviously shown to be correlated with the perceived texture of reverberation,\nis also close to that of IM. However, its computational complexity is one to\ntwo orders of magnitude lower, comparable to the computational complexity of a\nfeedback delay network (FDN), and its memory requirements are negligible.\n", "subjects": "Multimedia (cs.MM)", "authors": "Enzo De Sena, Huseyin Hacihabiboglu, Zoran Cvetkovic, Julius O. Smith III,", "date": "2015-02-24 01:36:22.683685"}, 
{"urllink": "http://arxiv.org/abs/1502.05968", "category": "Networking and Internet Architecture ", "pdflink": "http://arxiv.org/pdf/1502.05968", "title": "\nScheduling Storms and Streams in the Cloud", "abstract": "> Motivated by emerging big streaming data processing paradigms (e.g., Twitter\nStorm, Streaming MapReduce), we investigate the problem of scheduling graphs\nover a large cluster of servers. Each graph is a job, where nodes represent\ncompute tasks and edges indicate data-flows between these compute tasks. Jobs\n(graphs) arrive randomly over time, and upon completion, leave the system. When\na job arrives, the scheduler needs to partition the graph and distribute it\nover the servers to satisfy load balancing and cost considerations.\nSpecifically, neighboring compute tasks in the graph that are mapped to\ndifferent servers incur load on the network; thus a mapping of the jobs among\nthe servers incurs a cost that is proportional to the number of \"broken edges\".\nWe propose a low complexity randomized scheduling algorithm that, without\nservice preemptions, stabilizes the system with graph arrivals/departures; more\nimportantly, it allows a smooth trade-off between minimizing average\npartitioning cost and average queue lengths. Interestingly, to avoid service\npreemptions, our approach does not rely on a Gibbs sampler; instead, we show\nthat the corresponding limiting invariant measure has an interpretation\nstemming from a loss system.\n", "subjects": "Networking and Internet Architecture (cs.NI)", "authors": "Javad Ghaderi, Sanjay Shakkottai, R Srikant,", "date": "2015-02-24 01:36:22.878538"}, 
{"urllink": "http://arxiv.org/abs/1502.05777", "category": "Neural and Evolutionary Computing ", "pdflink": "http://arxiv.org/pdf/1502.05777", "title": "\nSpike Event Based Learning in Neural Networks", "abstract": "> A scheme is derived for learning connectivity in spiking neural networks. The\nscheme learns instantaneous firing rates that are conditional on the activity\nin other parts of the network. The scheme is independent of the choice of\nneuron dynamics or activation function, and network architecture. It involves\ntwo simple, online, local learning rules that are applied only in response to\noccurrences of spike events. This scheme provides a direct method for\ntransferring ideas between the fields of deep learning and computational\nneuroscience. This learning scheme is demonstrated using a layered feedforward\nspiking neural network trained self-supervised on a prediction and\nclassification task for moving MNIST images collected using a Dynamic Vision\nSensor.\n", "subjects": "Neural and Evolutionary Computing (cs.NE)", "authors": "James A. Henderson, TingTing A. Gibson, Janet Wiles,", "date": "2015-02-24 01:36:23.211613"}, 
{"urllink": "http://arxiv.org/abs/1502.05880", "category": "Numerical Analysis ", "pdflink": "http://arxiv.org/pdf/1502.05880", "title": "\nA Flexible Implementation of a Matrix Laurent Series-Based 16-Point Fast  Fourier and Hartley Transforms", "abstract": "> This paper describes a flexible architecture for implementing a new fast\ncomputation of the discrete Fourier and Hartley transforms, which is based on a\nmatrix Laurent series. The device calculates the transforms based on a single\nbit selection operator. The hardware structure and synthesis are presented,\nwhich handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E\ndevice.\n", "subjects": "Numerical Analysis (cs.NA)", "authors": "R.C. de Oliveira, H.M. de Oliveira, R.M. Campello de Souza, E.J.P. Santos,", "date": "2015-02-24 01:36:23.041283"}, 
{"urllink": "http://arxiv.org/abs/1502.02287", "category": "Operating Systems ", "pdflink": "http://arxiv.org/pdf/1502.02287", "title": "\nProtecting Memory-Performance Critical Sections in Soft Real-Time  Applications", "abstract": "> Soft real-time applications such as multimedia applications often show bursty\nmemory access patterns---regularly requiring a high memory bandwidth for a\nshort duration of time. Such a period is often critical for timely data\nprocessing. Hence, we call it a memory-performance critical section.\nUnfortunately, in multicore architecture, non-real-time applications on\ndifferent cores may also demand high memory bandwidth at the same time, which\ncan substantially increase the time spent on the memory performance critical\nsections.\n<br>In this paper, we present BWLOCK, user-level APIs and a memory bandwidth\ncontrol mechanism that can protect such memory performance critical sections of\nsoft real-time applications. BWLOCK provides simple lock like APIs to declare\nmemory-performance critical sections. If an application enters a\nmemory-performance critical section, the memory bandwidth control system then\ndynamically limit other cores' memory access rates to protect memory\nperformance of the application until the critical section finishes.\n<br>From case studies with real-world soft real-time applications, we found (1)\nsuch memory-performance critical sections do exist and are often easy to\nidentify; and (2) applying BWLOCK for memory critical sections significantly\nimprove performance of the soft real-time applications at a small or no cost in\nthroughput of non real-time applications.\n", "subjects": "Operating Systems (cs.OS)", "authors": "Heechul Yun, Santosh Gondi, Siddhartha Biswas,", "date": "2015-02-24 01:36:23.389555"}, 
{"urllink": "http://arxiv.org/abs/1502.05931", "category": "Other Computer Science ", "pdflink": "http://arxiv.org/pdf/1502.05931", "title": "\nImproved Model for Wire-Length Estimation in Stochastic Wiring  Distribution", "abstract": "> This paper presents a pair of improved stochastic wiring distribution model\nfor better estimation of on-chip wire lengths. The proposed models provide 28 -\n50% reduction in error when estimating the average on-chip wire length compared\nto the estimation using the existing models. The impact of Rent's exponent on\nthe average wire length estimation is also investigated to demonstrate\nlimitations of the approximations used in some of the current models. To\nimprove the approximations of the model a new threshold for Rent's constant is\nrecommended. Simulation results demonstrate that proposed models with the new\nthreshold reduce the error of estimation by 38 to 75 percent compared to the\nprevious works.\n", "subjects": "Other Computer Science (cs.OH)", "authors": "Mohamed S. Hefeida, Masud H. Chowdhury,", "date": "2015-02-24 01:36:23.383357"}, 
{"urllink": "http://arxiv.org/abs/1502.05906", "category": "Other Computer Science ", "pdflink": "http://arxiv.org/pdf/1502.05906", "title": "\nConverting ECG and other paper legated biomedical maps into digital  signals", "abstract": "> This paper presents a digital signal processing tool developed using\nMatlabTM, which provides a very low-cost and effective strategy for\nanalog-to-digital conversion of legated paper biomedical maps without requiring\ndedicated hardware. This software-based approach is particularly helpful for\ndigitalizing biomedical signals acquired from analogical devices equipped with\na plottingter. Albeit signals used in biomedical diagnosis are the primary\nconcern, this imaging processing tool is suitable to modernize facilities in a\nnon-expensive way. Legated paper ECG and EEG charts can be fast and efficiently\ndigitalized in order to be added in existing up-to-date medical data banks,\nimproving the follow-up of patients.\n", "subjects": "Other Computer Science (cs.OH)", "authors": "A.R. Gomes e Silva, H.M. de Oliveira, R.D. Lins,", "date": "2015-02-24 01:36:23.382485"}, 
{"urllink": "http://arxiv.org/abs/1502.05887", "category": "Other Computer Science ", "pdflink": "http://arxiv.org/pdf/1502.05887", "title": "\nAnalysis of Lithography Based Approaches In deveopment of Semi  Conductors", "abstract": "> The end of the 19th century brought about a change in the dynamics of\ncomputing by the development of the microprocessor. Huge bedroom size computers\nbegan being replaced by portable, smaller sized desktops. Today the world is\ndominated by silicon, which has circumscribed chip development for computers\nthrough microprocessors. Majority of the integrated circuits that are\nmanufactured at present are developed using the concept of Lithography. This\npaper presents a detailed analysis of multiple Lithography methodologies as a\nmeans for advanced integrated circuit development. The study paper primarily\nrestricts to examples in the context of Lithography, surveying the various\nexisting techniques of Lithography in literature, examining feasible and\nefficient methods, highlighting the various pros and cons of each of them.\n", "subjects": "Other Computer Science (cs.OH)", "authors": "Jatin Chopra,", "date": "2015-02-24 01:36:23.381692"}, 
{"urllink": "http://arxiv.org/abs/1502.05094", "category": "Programming Languages ", "pdflink": "http://arxiv.org/pdf/1502.05094", "title": "\nObservationally Cooperative Multithreading", "abstract": "> Despite widespread interest in multicore computing, concur- rency models in\nmainstream languages often lead to subtle, error-prone code.\n<br>Observationally Cooperative Multithreading (OCM) is a new approach to\nshared-memory parallelism. Programmers write code using the well-understood\ncooperative (i.e., nonpreemptive) multithreading model for uniprocessors. OCM\nthen allows threads to run in parallel, so long as results remain consistent\nwith the cooperative model.\n<br>Programmers benefit because they can reason largely sequentially. Remaining\ninterthread interactions are far less chaotic than in other models, permitting\neasier reasoning and debugging. Programmers can also defer the choice of\nconcurrency-control mechanism (e.g., locks or transactions) until after they\nhave written their programs, at which point they can compare\nconcurrency-control strategies and choose the one that offers the best\nperformance. Implementers and researchers also benefit from the agnostic nature\nof OCM -- it provides a level of abstraction to investigate, compare, and\ncombine a variety of interesting concurrency-control techniques.\n", "subjects": "Programming Languages (cs.PL)", "authors": "Christopher A. Stone, Melissa E. O'Neill, Sonja A. Bohr, Adam M. Cozzette, M. Joe DeBlasio, Julia Matsieva, Stuart A. Pernsteiner, Ari D. Schumer,", "date": "2015-02-24 01:36:23.554423"}, 
{"urllink": "http://arxiv.org/abs/1502.05818", "category": "Networking and Internet Architecture ", "pdflink": "http://arxiv.org/pdf/1502.05818", "title": "\nCo-Primary Multi-Operator Resource Sharing for Small Cell Networks", "abstract": "> To tackle the challenge of providing higher data rates within limited\nspectral resources we consider the case of multiple operators sharing a common\npool of radio resources. Four algorithms are proposed to address co-primary\nmulti-operator radio resource sharing under heterogeneous traffic in both\ncentralized and distributed scenarios. The performance of these algorithms is\nassessed through extensive system-level simulations for two indoor small cell\nlayouts. It is assumed that the spectral allocations of the small cells are\northogonal to the macro network layer and thus, only the small cell traffic is\nmodeled. The main performance metrics are user throughput and the relative\namount of shared spectral resources. The numerical results demonstrate the\nimportance of coordination among co-primary operators for an optimal resource\nsharing. Also, maximizing the spectrum sharing percentage generally improves\nthe achievable throughput gains over non-sharing.\n", "subjects": "Networking and Internet Architecture (cs.NI)", "authors": "Petri Luoto, Pekka Pirinen, Mehdi Bennis, Sumudu Samarakoon, Simon Scott, Matti Latva-aho,", "date": "2015-02-24 01:36:22.877903"}, 
{"urllink": "http://arxiv.org/abs/1502.05817", "category": "Networking and Internet Architecture ", "pdflink": "http://arxiv.org/pdf/1502.05817", "title": "\nA Hybrid Model to Extend Vehicular Intercommunication V2V through D2D  Architecture", "abstract": "> In the recent years, many solutions for Vehicle to Vehicle (V2V)\ncommunication were proposed to overcome failure problems (also known as dead\nends). This paper proposes a novel framework for V2V failure recovery using\nDevice-to-Device (D2D) communications. Based on the unified Intelligent\nTransportation Systems (ITS) architecture, LTE-based D2D mechanisms can improve\nV2V dead ends failure recovery delays. This new paradigm of hybrid V2V-D2D\ncommunications overcomes the limitations of traditional V2V routing techniques.\nAccording to NS2 simulation results, the proposed hybrid model decreases the\nend to end delay (E2E) of messages delivery. A complete comparison of different\nD2D use cases (best &amp; worst scenarios) is presented to show the enhancements\nbrought by our solution compared to traditional V2V techniques.\n", "subjects": "Networking and Internet Architecture (cs.NI)", "authors": "Emad Abd-Elrahman, Adel Mounir Said, Thouraya Toukabri, Hossam Afifi, Michel Marot,", "date": "2015-02-24 01:36:22.877176"}, 
{"urllink": "http://arxiv.org/abs/1502.05834", "category": "Logic in Computer Science ", "pdflink": "http://arxiv.org/pdf/1502.05834", "title": "\nBimodal logics with a `weakly connected' component without the finite  model property", "abstract": "> There are two known general results on the finite model property (fmp) of\ncommutators [L,L'] (bimodal logics with commuting and confluent modalities). If\nL is finitely axiomatisable by modal formulas having universal Horn first-order\ncorrespondents, then both [L,K] and [L,S5] are determined by classes of frames\nthat admit filtration, and so have the fmp. On the negative side, if both L and\nL' are determined by transitive frames and have frames of arbitrarily large\ndepth, then [L,L'] does not have the fmp. In this paper we show that\ncommutators with a `weakly connected' component often lack the fmp. Our results\nimply that the above positive result does not generalise to universally\naxiomatisable component logics, and even commutators without `transitive'\ncomponents such as [K.3,K] can lack the fmp. We also generalise the above\nnegative result to cases where one of the component logics has frames of depth\none only, such as [S4.3,S5] and the decidable product logic S4.3xS5. We also\nshow cases when already half of commutativity is enough to force infinite\nframes.\n", "subjects": "Logic in Computer Science (cs.LO)", "authors": "Agi Kurucz,", "date": "2015-02-24 01:36:22.527165"}, 
{"urllink": "http://arxiv.org/abs/1502.05748", "category": "Logic in Computer Science ", "pdflink": "http://arxiv.org/pdf/1502.05748", "title": "\nApplying Fuzzy Logic to the Design, Verification and Analysis of Binary  Hardware Circuits", "abstract": "> We present a novel approach for digital hardware simulation based on\nmany-valued (fuzzy) logic (MVL). Binary designs can be automatically\ntransformed into MVL designs, and simulations performed in the more informative\nMVL setting may reveal details which are either invisible or hard to detect\nthrough binary simulations. Two circuits which are supposed to be binary\nequivalent may behave differently under MVL simulations, and analyzing these\ndifferences may lead to the discovery of a genuine binary nonequivalence, or in\nsome cases, to a qualitative gap between the designs. By performing an MVL\nsimulation, a combinational design becomes a union of trajectories, where each\ntrajectory starts at some input variable and all the nodes along the trajectory\nare of the same degree of veracity or falsehood. With sequential synchronous\ndesigns one can incorporate temporal data into the simulation, so that the\nstate of the design at a given time reports besides the degree of truth of each\nvariable also the place and date of birth of its value. Applications include\nequivalence verification, initialization, assertions generation and\nverification, stuck-at-values, partial control on the flow of data by\nprioritizing, block-oriented simulations. Some procedures and general\ndirections towards achieving these goals are presented.\n", "subjects": "Logic in Computer Science (cs.LO)", "authors": "Amnon Rosenmann,", "date": "2015-02-24 01:36:22.526383"}, 
{"urllink": "http://arxiv.org/abs/1502.05911", "category": "Learning ", "pdflink": "http://arxiv.org/pdf/1502.05911", "title": "\nA Data Mining framework to model Consumer Indebtedness with  Psychological Factors", "abstract": "> Modelling Consumer Indebtedness has proven to be a problem of complex nature.\nIn this work we utilise Data Mining techniques and methods to explore the\nmultifaceted aspect of Consumer Indebtedness by examining the contribution of\nPsychological Factors, like Impulsivity to the analysis of Consumer Debt. Our\nresults confirm the beneficial impact of Psychological Factors in modelling\nConsumer Indebtedness and suggest a new approach in analysing Consumer Debt,\nthat would take into consideration more Psychological characteristics of\nconsumers and adopt techniques and practices from Data Mining.\n", "subjects": "Learning (cs.LG)", "authors": "Alexandros Ladas, Eamonn Ferguson, Uwe Aickelin, Jon Garibaldi,", "date": "2015-02-24 01:36:22.255634"}, 
{"urllink": "http://arxiv.org/abs/1502.05890", "category": "Learning ", "pdflink": "http://arxiv.org/pdf/1502.05890", "title": "\nEfficient Contextual Semi-Bandit Learning", "abstract": "> We study a variant of the contextual bandit problem, where on each round, the\nlearner plays a sequence of actions, receives a feature for each individual\naction, and reward that is linearly related to these features. This setting has\napplications to network routing, crowd-sourcing, personalized search, and many\nother domains. If the linear transformation is known, we analyze an algorithm\nthat is structurally similar to the algorithm of Agarwal et a. [2014] and show\nthat it enjoys a regret bound between $\\tilde{O}(\\sqrt{KLT \\ln N})$ and\n$\\tilde{O}(L\\sqrt{KT \\ln N})$, where $K$ is the number of actions, $L$ is the\nlength of each action sequence, $T$ is the number of rounds, and $N$ is the\nnumber of policies. If the linear transformation is unknown, we show that an\nalgorithm that first explores to learn the unknown weights via linear\nregression and thereafter uses the estimated weights can achieve\n$\\tilde{O}(\\|w\\|_1(KT)^{3/4} \\sqrt{\\ln N})$ regret, where $w$ is the true\n(unknown) weight vector. Both algorithms use an optimization oracle to avoid\nexplicit enumeration of the policies and consequently are computationally\nefficient whenever an efficient algorithm for the fully supervised setting is\navailable.\n", "subjects": "Learning (cs.LG)", "authors": "Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudik,", "date": "2015-02-24 01:36:22.254587"}, 
{"urllink": "http://arxiv.org/abs/1502.05216", "category": "Mathematical Software ", "pdflink": "http://arxiv.org/pdf/1502.05216", "title": "\nTwofold exp and log", "abstract": "> This article is about twofold arithmetic. Here I introduce algorithms and\nexperimental code for twofold variant of C/C++ standard functions exp() and\nlog(), and expm1() and log1p(). Twofold function $y_0+y_1 \\approx f(x_0+x_1)$\nis nearly 2x-precise so can assess accuracy of standard one. Performance allows\nassessing on-fly: twofold texp() over double is ~10x times faster than expq()\nby GNU quadmath.\n", "subjects": "Mathematical Software (cs.MS)", "authors": "Evgeny Latkin,", "date": "2015-02-24 01:36:22.521574"}, 
{"urllink": "http://arxiv.org/abs/1502.05832", "category": "Learning ", "pdflink": "http://arxiv.org/pdf/1502.05832", "title": "\nA provably convergent alternating minimization method for mean field  inference", "abstract": "> Mean-Field is an efficient way to approximate a posterior distribution in\ncomplex graphical models and constitutes the most popular class of Bayesian\nvariational approximation methods. In most applications, the mean field\ndistribution parameters are computed using an alternate coordinate\nminimization. However, the convergence properties of this algorithm remain\nunclear. In this paper, we show how, by adding an appropriate penalization\nterm, we can guarantee convergence to a critical point, while keeping a closed\nform update at each step. A convergence rate estimate can also be derived based\non recent results in non-convex optimization.\n", "subjects": "Learning (cs.LG)", "authors": "Pierre Baqu\u00e9, Jean-Hubert Hours, Fran\u00e7ois Fleuret, Pascal Fua,", "date": "2015-02-24 01:36:22.253450"}, 
{"urllink": "http://arxiv.org/abs/1502.05881", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05881", "title": "\nOrthogonal Multilevel Spreading Sequence Design", "abstract": "> Finite field transforms are offered as a new tool of spreading sequence\ndesign. This approach exploits orthogonality properties of synchronous\nnon-binary sequences defined over a complex finite field. It is promising for\nchannels supporting a high signal-to-noise ratio. New digital multiplex schemes\nbased on such sequences have also been introduced, which are multilevel Code\nDivision Multiplex. These schemes termed Galois-field Division Multiplex (GDM)\nare based on transforms for which there exists fast algorithms. They are also\nconvenient from the hardware viewpoint since they can be implemented by a\nDigital Signal Processor. A new Efficient-bandwidth\ncode-division-multiple-access (CDMA) is introduced, which is based on\nmultilevel spread spectrum sequences over a Galois field. The primary advantage\nof such schemes regarding classical multiple access digital schemes is their\nbetter spectral efficiency. Galois-Fourier transforms contain some redundancy\nand only cyclotomic coefficients are needed to be transmitted yielding compact\nspectrum requirements.\n", "subjects": "Information Theory (cs.IT)", "authors": "H.M. de Oliveira, R.M. Campello de Souza,", "date": "2015-02-24 01:36:22.103360"}, 
{"urllink": "http://arxiv.org/abs/1502.05744", "category": "Learning ", "pdflink": "http://arxiv.org/pdf/1502.05744", "title": "\nScale-Free Algorithms for Online Linear Optimization", "abstract": "> Online linear optimization problem models a situation where an algorithm\nrepeatedly has to make a decision before it sees the loss function, which is a\nlinear function of the decision. The performance of an algorithm is measured by\nthe so-called regret, which is the difference between the cumulative loss of\nthe algorithm and the cumulative loss of the best fixed decision in hindsight.\n<br>We present algorithms for online linear optimization that are oblivious to\nthe scaling of the losses. That is, the algorithms make the exactly the same\nsequence of decisions if the loss functions are scaled by an arbitrary positive\nconstant. Consequence of the scale invariance is that the algorithms do not\nneed to know an upper bound on the norm of the loss vectors as an input. The\nbound on algorithms' regret is within constant factor of the previously known\noptimal bound for the situation where the scaling is known.\n<br>Coincidentally, the algorithms do not need to know the number of rounds and\ndo not have any other tuning parameters.\n", "subjects": "Learning (cs.LG)", "authors": "Francesco Orabona, David Pal,", "date": "2015-02-24 01:36:22.252316"}, 
{"urllink": "http://arxiv.org/abs/1502.05879", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05879", "title": "\nWavelet Analysis as an Information Processing Technique", "abstract": "> A new interpretation for the wavelet analysis is reported, which can is\nviewed as an information processing technique. It was recently proposed that\nevery basic wavelet could be associated with a proper probability density,\nallowing defining the entropy of a wavelet. Introducing now the concept of\nwavelet mutual information between a signal and an analysing wavelet fulfils\nthe foundations of a wavelet information theory (WIT). Both continuous and\ndiscrete time signals are considered. Finally, we showed how to compute the\ninformation provided by a multiresolution analysis by means of the\ninhomogeneous wavelet expansion. Highlighting ideas behind the WIT are\npresented.\n", "subjects": "Information Theory (cs.IT)", "authors": "H.M. de Oliveira, D.F. de Souza,", "date": "2015-02-24 01:36:22.102386"}, 
{"urllink": "http://arxiv.org/abs/1502.05871", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05871", "title": "\nRobust CS reconstruction based on appropriate minimization norm", "abstract": "> Noise robust compressive sensing algorithm is considered. This algorithm\nallows an efficient signal reconstruction in the presence of different types of\nnoise due to the possibility to change minimization norm. For instance, the\ncommonly used l1 and l2 norms, provide good results in case of Laplace and\nGaussian noise. However, when the signal is corrupted by Cauchy or Cubic\nGaussian noise, these norms fail to provide accurate reconstruction. Therefore,\nin order to achieve accurate reconstruction, the application of l3 minimization\nnorm is analyzed. The efficiency of algorithm will be demonstrated on examples.\n", "subjects": "Information Theory (cs.IT)", "authors": "Maja Lakicevic, Mitar Moracanin, Nadja Djerkovic,", "date": "2015-02-24 01:36:22.101456"}, 
{"urllink": "http://arxiv.org/abs/1502.05808", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05808", "title": "\nMatrix Codes as Ideals for Grassmannian Codes and their Weight  Properties", "abstract": "> A systematic way of constructing Grassmannian codes endowed with the subspace\ndistance as lifts of matrix codes over the prime field $GF(p)$ is introduced.\nThe matrix codes are $GF(p)$-subspaces of the ring $M_2(GF(p))$ of $2 \\times 2$\nmatrices over $GF(p)$ on which the rank metric is applied, and are generated as\none-sided proper principal ideals by idempotent elements of $M_2(GF(p))$.\nFurthermore a weight function on the non-commutative matrix ring $M_2(GF(p))$,\n$q$ a power of $p$, is studied in terms of the egalitarian and homogeneous\nconditions. The rank weight distribution of $M_2(GF(q))$ is completely\ndetermined by the general linear group $GL(2,q)$. Finally a weight function on\nsubspace codes is analogously defined and its egalitarian property is examined.\n", "subjects": "Information Theory (cs.IT)", "authors": "Bryan Hernandez, Virgilio Sison,", "date": "2015-02-24 01:36:22.099585"}, 
{"urllink": "http://arxiv.org/abs/1502.05807", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05807", "title": "\nNoise-shaping Quantization Methods for Frame-based and Compressive  Sampling Systems", "abstract": "> Noise shaping refers to an analog-to-digital conversion methodology in which\nquantization error is arranged to lie mostly outside the signal spectrum by\nmeans of oversampling and feedback. Recently it has been successfully applied\nto more general redundant linear sampling and reconstruction systems associated\nwith frames as well as non-linear systems associated with compressive sampling.\nThis chapter reviews some of the recent progress in this subject.\n", "subjects": "Information Theory (cs.IT)", "authors": "Evan Chou, C. Sinan G\u00fcnt\u00fcrk, Felix Krahmer, Rayan Saab, \u00d6zg\u00fcr Y\u0131lmaz,", "date": "2015-02-24 01:36:22.097333"}, 
{"urllink": "http://arxiv.org/abs/1502.05784", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05784", "title": "\nLDPC Code Design for Noncoherent Physical Layer Network Coding", "abstract": "> This work considers optimizing LDPC codes in the physical-layer network coded\ntwo-way relay channel using noncoherent FSK modulation. The error-rate\nperformance of channel decoding at the relay node during the multiple-access\nphase was improved through EXIT-based optimization of Tanner graph variable\nnode degree distributions. Codes drawn from the DVB-S2 and WiMAX standards were\nused as a basis for design and performance comparison. The computational\ncomplexity characteristics of the standard codes were preserved in the\noptimized codes by maintaining the extended irregular repeat-accumulate (eIRA).\nThe relay receiver performance was optimized considering two modulation orders\nM = {4, 8} using iterative decoding in which the decoder and demodulator refine\nchannel estimates by exchanging information. The code optimization procedure\nyielded unique optimized codes for each case of modulation order and available\nchannel state information. Performance of the standard and optimized codes were\nmeasured using Monte Carlo simulation in the flat Rayleigh fading channel, and\nerror rate improvements up to 1.2 dB are demonstrated depending on system\nparameters.\n", "subjects": "Information Theory (cs.IT)", "authors": "Terry Ferrett, Matthew C. Valenti,", "date": "2015-02-24 01:36:22.095733"}, 
{"urllink": "http://arxiv.org/abs/1502.05782", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05782", "title": "\nImpact of Metro Cell Antenna Pattern and Downtilt in Heterogeneous  Networks", "abstract": "> This article discusses the positive impact metro cell antennas with narrow\nvertical beamwidth and electrical downtilt can have on heterogeneous cellular\nnetworks. Using a model of random cell placement based on Poisson distribution,\nalong with an innovative 3D building model that quantifies blockage due to\nshadowing, it is demonstrated that network spectral efficiency and average user\nthroughput both increase as vertical beamwidth is decreased and downtilt is\napplied to metro cell transmission. Moreover, the network becomes more energy\nefficient. Importantly, these additional gains in network performance can be\nachieved without any cooperation or exchange of information between macro cell\nbase stations and metro cells.\n", "subjects": "Information Theory (cs.IT)", "authors": "Xiao Li, Robert W. Heath Jr., Kevin Linehan, Ray Butler,", "date": "2015-02-24 01:36:22.094814"}, 
{"urllink": "http://arxiv.org/abs/1502.05773", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05773", "title": "\nMultipartite Monotones for Secure Sampling by Public Discussion From  Noisy Correlations", "abstract": "> We address the problem of quantifying the cryptographic content of\nprobability distributions, in relation to an application to secure multi-party\nsampling against a passive t-adversary. We generalize a recently introduced\nnotion of assisted common information of a pair of correlated sources to that\nof K sources and define a family of monotone rate regions indexed by K. This\nallows for a simple characterization of all t-private distributions that can be\nstatistically securely sampled without any auxiliary setup of pre-shared noisy\ncorrelations. We also give a new monotone called the residual total correlation\nthat admits a simple operational interpretation. Interestingly, for sampling\nwith non-trivial setups (K &gt; 2) in the public discussion model, our definition\nof a monotone region differs from the one by Prabhakaran and Prabhakaran (ITW\n2012).\n", "subjects": "Information Theory (cs.IT)", "authors": "Pradeep Kr. Banerjee,", "date": "2015-02-24 01:36:22.092814"}, 
{"urllink": "http://arxiv.org/abs/1502.05775", "category": "Information Theory ", "pdflink": "http://arxiv.org/pdf/1502.05775", "title": "\nCommon Information Duality in Relation to Bounds on the Secret Key Rate", "abstract": "> We explore the duality between the simulation and extraction of secret common\nrandomness in light of a similar well-known operational duality between the two\nnotions of common information due to Wyner, and G\\'acs and K\\\"{o}rner. For the\ninverse problem of simulating a tripartite noisy correlation from a noiseless\nsecret key and unlimited public communication, we show that Winter's (2005)\nsingle-letter result for the secret key cost of formation can be simply\nreexpressed in terms of the existence of a bipartite protocol monotone. For the\nforward problem of key distillation from noisy correlations without public\ndiscussion, we show that for a class of decomposable distributions, the\nconditional G\\'acs and K\\\"{o}rner common information achieves a tight bound on\nthe secret key rate.\n", "subjects": "Information Theory (cs.IT)", "authors": "Pradeep Kr. Banerjee,", "date": "2015-02-24 01:36:22.093854"}, 
{"urllink": "http://arxiv.org/abs/1502.04232", "category": "Graphics ", "pdflink": "http://arxiv.org/pdf/1502.04232", "title": "\nSketch-based Shape Retrieval using Pyramid-of-Parts", "abstract": "> We present a multi-scale approach to sketch-based shape retrieval. It is\nbased on a novel multi-scale shape descriptor called Pyramidof- Parts, which\nencodes the features and spatial relationship of the semantic parts of query\nsketches. The same descriptor can also be used to represent 2D projected views\nof 3D shapes, allowing effective matching of query sketches with 3D shapes\nacross multiple scales. Experimental results show that the proposed method\noutperforms the state-of-the-art method, whether the sketch segmentation\ninformation is obtained manually or automatically by considering each stroke as\na semantic part.\n", "subjects": "Graphics (cs.GR)", "authors": "Changqing Zou, Zhe Huang, Rynson W. H. Lau, Jianzhuang Liu, Hongbo Fu,", "date": "2015-02-24 01:36:21.752276"}, 
{"urllink": "http://arxiv.org/abs/1502.05623", "category": "Robotics ", "pdflink": "http://arxiv.org/pdf/1502.05623", "title": "\nPlanar Linkages Following a Prescribed Motion", "abstract": "> Designing mechanical devices, called linkages, that draw a given plane curve\nhas been a topic that interested engineers and mathematicians for hundreds of\nyears, and recently also computer scientists. Already in 1876, Kempe proposed a\nprocedure for solving the problem in full generality, but his constructions\ntend to be extremely complicated. We provide a novel algorithm that produces\nmuch simpler linkages, but works only for parametric curves. Our approach is to\ntransform the problem into a factorization task over some noncommutative\nalgebra. We show how to compute such a factorization, and how to use it to\nconstruct a linkage tracing a given curve.\n", "subjects": "Symbolic Computation (cs.SC)", "authors": "Matteo Gallet, Christoph Koutschan, Zijia Li, Georg Regensburger, Josef Schicho, Nelly Villamizar,", "date": "2015-02-24 01:36:28.333668"}, 
{"urllink": "http://arxiv.org/abs/1502.05886", "category": "Social and Information Networks ", "pdflink": "http://arxiv.org/pdf/1502.05886", "title": "\nOn predictability of rare events leveraging social media: a machine  learning perspective", "abstract": "> Information extracted from social media streams has been leveraged to\nforecast the outcome of a large number of real-world events, from political\nelections to stock market fluctuations. An increasing amount of studies\ndemonstrates how the analysis of social media conversations provides cheap\naccess to the wisdom of the crowd. However, extents and contexts in which such\nforecasting power can be effectively leveraged are still unverified at least in\na systematic way. It is also unclear how social-media-based predictions compare\nto those based on alternative information sources. To address these issues,\nhere we develop a machine learning framework that leverages social media\nstreams to automatically identify and predict the outcomes of soccer matches.\nWe focus in particular on matches in which at least one of the possible\noutcomes is deemed as highly unlikely by professional bookmakers. We argue that\nsport events offer a systematic approach for testing the predictive power of\nsocial media, and allow to compare such power against the rigorous baselines\nset by external sources. Despite such strict baselines, our framework yields\nabove 8% marginal profit when used to inform simple betting strategies. The\nsystem is based on real-time sentiment analysis and exploits data collected\nimmediately before the games, allowing for informed bets. We discuss the\nrationale behind our approach, describe the learning framework, its prediction\nperformance and the return it provides as compared to a set of betting\nstrategies. To test our framework we use both historical Twitter data from the\n2014 FIFA World Cup games, and real-time Twitter data collected by monitoring\nthe conversations about all soccer matches of four major European tournaments\n(FA Premier League, Serie A, La Liga, and Bundesliga), and the 2014 UEFA\nChampions League, during the period between Oct. 25th 2014 and Nov. 26th 2014.\n", "subjects": "Social and Information Networks (cs.SI)", "authors": "Lei Le, Emilio Ferrara, Alessandro Flammini,", "date": "2015-02-24 01:36:28.522479"}, 
{"urllink": "http://arxiv.org/abs/1502.05767", "category": "Symbolic Computation ", "pdflink": "http://arxiv.org/pdf/1502.05767", "title": "\nAutomatic differentiation in machine learning: a survey", "abstract": "> Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in\nmachine learning. Automatic differentiation (AD) is a technique for calculating\nderivatives efficiently and accurately, established in fields such as\ncomputational fluid dynamics, nuclear engineering, and atmospheric sciences.\nDespite its advantages and use in other fields, machine learning practitioners\nhave been little influenced by AD and make scant use of available tools. We\nsurvey the intersection of AD and machine learning, cover applications where AD\nhas the potential to make a big impact, and report on the recent developments\nin the adoption this technique. We also aim to dispel some misconceptions that\nwe think have impeded the widespread awareness of AD within the machine\nlearning community.\n", "subjects": "Symbolic Computation (cs.SC)", "authors": "Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul,", "date": "2015-02-24 01:36:28.996801"}, 
{"urllink": "http://arxiv.org/abs/1502.05789", "category": "Systems and Control ", "pdflink": "http://arxiv.org/pdf/1502.05789", "title": "\nLocation Identification of Power Line Outages Using PMU Measurements  with Bad Data", "abstract": "> The use of phasor angle measurements provided by phasor measurement units\n(PMUs) in fault detection is regarded as a promising method in identifying\nlocations of power line outages. However, communication errors or system\nmalfunctions may introduce errors to the measurements and thus yield bad data.\nMost of the existing methods on line outage identification fail to consider\nsuch error. This paper develops a framework for identifying multiple power line\noutages based on the PMUs' measurements in the presence of bad data. In\nparticular, we design an algorithm to identify locations of line outage and\nrecover the faulty measurements simultaneously. The proposed algorithm does not\nrequire any prior information on the number of line outages and the noise\nvariance. Case studies carried out on test systems of different sizes validate\nthe effectiveness and efficiency of the proposed approach.\n", "subjects": "Systems and Control (cs.SY)", "authors": "Wen-Tai Li, Chao-Kai Wen, Jung-Chieh Chen, Kai-Kit Wong, Jen-Hao Teng, Chau Yuen,", "date": "2015-02-24 01:36:29.034389"}, 
